---
title: "Lab4_report"
author: "Gustav Wahlquist"
date: '2020-10-16'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TASK 1.1
*Write your own code for simulating from the posterior distribution of f using the squared
exponential kernel. The function (name it posteriorGP) should return a vector with
the posterior mean and variance of f, both evaluated at a set of x-values. You
can assume that the prior mean of f is zero for all x. The function should have the
following inputs: <br />

- X: Vector of training inputs.<br />
- y: Vector of training targets/outputs.<br />
- XStar: Vector of inputs where the posterior distribution is evaluated.<br />
- sigmaNoise: Noise standard deviation.<br />
- k: Covariance function or kernel. That is, the kernel should be a separate function.*

```{r echo=TRUE, eval=TRUE}
# Gaussian Regression Model
# INPUTS:
#   X: Vector of training inputs.
#   y: Vector of training targets/outputs.
#   XStar: Vector of inputs where the posterior distribution is evaluated.
#   sigmaNoise: Noise standard deviation.
#   k: Covariance function or kernel. That is, the kernel should be a separate function
posteriorGP <- function(X, y, XStar, sigmaNoise, k, sigmaf, ell){
  
  # Compute covariance matrix K(X,X)
  covM = k(X, X, sigmaf, ell)
  
  # Identity matrix 
  I = diag(dim(covM)[1])
  
  # Transposing the output from the cholesky-function since the implementation in R returns 
  # an upper triangular matrix and we want a lower triangular matrix. 
  L = t(chol(covM + (sigmaNoise**2)*I))
  
  # Compute covariance matrix K(X,XStar)
  KStar = k(X, XStar, sigmaf, ell)
  # Predictive mean
  alpha = solve(t(L), solve(L, y))
  FStarMean = t(KStar)%*%alpha
  
  # Compute covariance matrix K(XStar,XStar)
  KStar2 = k(XStar, XStar, sigmaf, ell)
  # Predictive variance
  v = solve(L, KStar)
  V = diag(KStar2 - t(v)%*%v)
  
  return(list('posteriorMean' = FStarMean, 'posteriorVariance' = V))
  
}
```
# TASK 1.2
*Now, let the prior hyperparameters be sigmaf = 1 and ell = 0.3. Update this prior with a single
observation: (x, y) = (0.4, 0.719). Assume that sigmaNoise = 0.1. Plot the posterior mean of f
over the interval x E [-1, 1]. Plot also 95 % probability (pointwise) bands for f.*

```{r echo=FALSE, eval=TRUE}
library("mvtnorm")

# Covariance function
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA,n1,n2)
  for (i in 1:n2){
    K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
  }
  return(K)
}

# Function for plotting posterior distributions with 95% probability band.
# INPUTS:   
#   means: Computed means for a number of points
#   variances: Computed variances for a number of points 
#   x: x-values for observations
#   y: y-values for observations
#   XStar: Vector of inputs where the posterior distribution is evaluated.
plotPosterior <- function(means, variances, x, y, XStar){
  upperBand = means + sqrt(variances)*1.96
  lowerBand = means - sqrt(variances)*1.96
  
  plot(x, y, type="p", 
       xlim=c(min(XStar),
              max(XStar)),
       ylim=c(min(min(y), min(lowerBand)),
              max(max(y), max(upperBand))))
  lines(XStar, means, col = "red", lwd = 3)
  lines(XStar, upperBand, col = "blue", lwd = 2)
  lines(XStar, lowerBand, col = "blue", lwd = 2)
  points(x, y, col='green')
}


# sigma_f: ????
sigmaf = 1
#The length scale, i.e how highly correlated function values should be for close input points (smoothing factor): 
ell = 0.3
# Observation:
x1 = 0.4
y1 = 0.719
# Evaluation points in interval [-1,1]:
XStar = seq(-1, 1, 0.01)

posterior1 = posteriorGP(x1, y1 , XStar, sigmaNoise = 0.1, SquaredExpKernel, sigmaf = sigmaf, ell = ell)
plotPosterior(posterior1$posteriorMean, posterior1$posteriorVariance, x1, y1, XStar)

```

# TASK 1.3
*Update your posterior from (1.2) with another observation: (x, y) = (-0.6,-0.044). Plot
the posterior mean of f over the interval x E [-1, 1]. Plot also 95 % probability (pointwise) bands for f.
Hint: Updating the posterior after one observation with a new observation gives the
same result as updating the prior directly with the two observations
*

```{r echo=FALSE, eval=TRUE}
# Another observation: 
x2 = -0.6
y2 = -0.044
X = c(x1, x2)
Y = c(y1, y2)

posterior2 = posteriorGP(X, Y, XStar, sigmaNoise = 0.1, SquaredExpKernel, sigmaf = sigmaf, ell = ell)
plotPosterior(posterior2$posteriorMean, posterior2$posteriorVariance, X, Y, XStar)

```

# TASK 1.4
*Compute the posterior distribution of f using all the five data points in the table below
(note that the two previous observations are included in the table). Plot the posterior
mean of f over the interval x E [-1, 1]. Plot also 95 % probability (pointwise) bands for
f. <br />
x -1.0 -0.6 -0.2 0.4 0.8 <br />
y 0.768 -0.044 -0.940 0.719 -0.664 <br />
*
```{r echo=FALSE, eval=TRUE}
# Observations: 
X = c(-1.0, -0.6, -0.2, 0.4, 0.8)
Y = c(0.768, -0.044, -0.940, 0.719, -0.664)

posterior2 = posteriorGP(X, Y, XStar, sigmaNoise = 0.1, SquaredExpKernel, sigmaf = sigmaf, ell = ell)
plotPosterior(posterior2$posteriorMean, posterior2$posteriorVariance, X, Y, XStar)
```

# TASK 1.5
*Repeat (4), this time with hyperparameters sigmaf = 1 and ell = 1. Compare the results.*

```{r echo=FALSE, eval=TRUE}
X = c(-1.0, -0.6, -0.2, 0.4, 0.8)
Y = c(0.768, -0.044, -0.940, 0.719, -0.664)
ell = 1

posterior2 = posteriorGP(X, Y, XStar, sigmaNoise = 0.1, SquaredExpKernel, sigmaf = sigmaf, ell = ell)
plotPosterior(posterior2$posteriorMean, posterior2$posteriorVariance, X, Y, XStar)

```

As expected, we see that this posterior over f is much smoother than the one in the task before. This is since we use a ell = 1 instead of ell = 0.3. The posterior with ell = 0.3 seems to resemble the few data points we have, however, depending on how the real distribution looks like, this model could be overfitted. The posterior with ell = 1 looks like it is underfitted when looking at the data points we have. 

# TASK 2.1 
*Create the variable time which records the day number since the start of the dataset (i.e.,
time= 1, 2, . . ., 365 Ã— 6 = 2190). Also, create the variable day that records the day number
since the start of each year (i.e., day= 1, 2, . . ., 365, 1, 2, . . ., 365). Estimating a GP on 2190
observations can take some time on slower computers, so let us subsample the data and use
only every fifth observation. This means that your time and day variables are now time= 1, 6,
11, . . ., 2186 and day= 1, 6, 11, . . ., 361, 1, 6, 11, . . ., 361.*
<br /> <br />

*Define your own square exponential kernel function (with parameters ell and sigmaf
(sigmaf)), evaluate it in the point x = 1, x' = 2, and use the kernelMatrix function
to compute the covariance matrix K(X, Xstar) for the input vectors X = T(1, 3, 4) and
Xstar = T(2, 3, 4).
*

```{r echo=TRUE, eval=TRUE}
data = read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/
Code/TempTullinge.csv", header=TRUE, sep=";")

# Creating variables with index for every fifth reading
tempTime = 1:2190
time = tempTime[seq(1,2190,5)]
dayNr = 1:365
tempDay = c()
for(i in 1:6){
  tempDay = c(tempDay, dayNr)
}
day = tempDay[seq(1,2190,5)]
temperature = unlist(data['temp'])
tempSubset = temperature[time]

# Returns a square exponential kernel function
# INPUTS:
#   sigmaf: standard deviation for f
#   ell: Smoothing factor
squareExpKernel <- function(sigmaf = 1, ell = 1) 
{
  rval <- function(x, y = NULL) {
    r = crossprod(x-y);
    #return((sigmaf**2)*exp(-r/2*(ell**2)))
    return(sigmaf^2 * exp(-0.5 * r / ell^2))
  }
  class(rval) <- "kernel"
  return(rval)
  
}
```

```{r echo=FALSE, eval=TRUE}
library(kernlab)

# Code from earlier part of lab
library("mvtnorm")

# Covariance function
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA,n1,n2)
  for (i in 1:n2){
    K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
  }
  return(K)
}

# Gaussian Regression Model
# INPUTS:
#   X: Vector of training inputs.
#   y: Vector of training targets/outputs.
#   XStar: Vector of inputs where the posterior distribution is evaluated.
#   sigmaNoise: Noise standard deviation.
#   k: Covariance function or kernel. That is, the kernel should be a separate function
posteriorGP <- function(X, y, XStar, sigmaNoise, k, sigmaf, ell){
  
  # Compute covariance matrix K(X,X)
  covM = k(X, X, sigmaf, ell)
  
  # Identity matrix 
  I = diag(dim(covM)[1])
  
  # Transposing the output from the cholesky-function since the implementation in R returns 
  # an upper triangular matrix and we want a lower triangular matrix. 
  L = t(chol(covM + (sigmaNoise**2)*I))
  
  # Compute covariance matrix K(X,XStar)
  KStar = k(X, XStar, sigmaf, ell)
  # Predictive mean
  alpha = solve(t(L), solve(L, y))
  FStarMean = t(KStar)%*%alpha
  
  # Compute covariance matrix K(XStar,XStar)
  KStar2 = k(XStar, XStar, sigmaf, ell)
  # Predictive variance
  v = solve(L, KStar)
  V = diag(KStar2 - t(v)%*%v)
  
 
  
  return(list('posteriorMean' = FStarMean, 'posteriorVariance' = V))
  
}

# Function for plotting posterior distributions with 95% probability band.
# INPUTS:   
#   means: Computed means for a number of points
#   variances: Computed variances for a number of points 
#   x: x-values for observations
#   y: y-values for observations
#   XStar: Vector of inputs where the posterior distribution is evaluated.
plotPosterior <- function(means, variances, x, y, XStar){
  var = sd(means) * variances + means
  upperBand = means + sqrt(var)*1.96
  lowerBand = means - sqrt(var)*1.96
  
  plot(x, y, type="p",
       xlim = c(min(x), max(x)), 
       ylim = c((min(min(lowerBand), min(y))), max(max(upperBand), max(y))))
  lines(XStar, means, col = "red", lwd = 2)
  lines(XStar, upperBand, col = "blue", lwd = 2)
  lines(XStar, lowerBand, col = "blue", lwd = 2)
  
}


# Creating kernel with sigmaf = 1, ell = 1
kernel = squareExpKernel(1,1)
# Evaluating in point x = 1, x' = 2
covM = kernel(1,2)

# Computing kernel matrix for created kernel and vectors X, XStar
X = c(1, 3, 4)
XStar = c(2, 3 ,4)
kernelMatrix = kernelMatrix(kernel = kernel, X, XStar)
kernelMatrix
```

# TASK 2.2 & 2.3
*Consider first the following model:*

![Model](C:\Users\gusta\Desktop\Studier\TDDE15\Lab 4\eq1.png)

*Estimate the above Gaussian process regression model using the squared
exponential function from (1) with sigmaf = 20 and ell = 0.2. Use the predict function in
R to compute the posterior mean at every data point in the training dataset. Make
a scatterplot of the data and superimpose the posterior mean of f as a curve (use
type="l" in the plot function).*

```{r echo=FALSE, eval=TRUE}
#
lmFit = lm(tempSubset ~ time + time**2) # vad Ã¤r detta exakt?
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell = 0.2
kernel = squareExpKernel(sigmaf = sigmaf, ell = ell)

#
fitGP = gausspr(x = time, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredict = predict(fitGP, time)

#
posterior = posteriorGP(X = scale(time), 
                        y = scale(tempSubset), 
                        XStar = scale(time), 
                        sigmaNoise = noise, 
                        k = SquaredExpKernel, 
                        sigmaf = sigmaf, 
                        ell = ell)

# Plotting results
plotPosterior(means = meanPredict, 
              variances = posterior$posteriorVariance,
              x = time,
              y = tempSubset,
              XStar = time)

```

# TASK 2.4
*Consider first the following model:*

![Model](C:\Users\gusta\Desktop\Studier\TDDE15\Lab 4\eq2.png)

*Estimate the model using the squared exponential function with sigmaf = 20 and ell = 0.2.
Superimpose the posterior mean from this model on the posterior mean from the model
in (2). Note that this plot should also have the time variable on the horizontal axis.
Compare the results of both models. What are the pros and cons of each model?*

```{r echo=FALSE, eval=TRUE}
#
lmFit = lm(tempSubset ~ day + day**2) # vad Ã¤r detta exakt?
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell = 0.2
kernel = squareExpKernel(sigmaf = sigmaf, ell = ell)

#
fitGP = gausspr(x = day, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredictDay = predict(fitGP, day)

#
posteriorDay = posteriorGP(X = scale(day), 
                        y = scale(tempSubset), 
                        XStar = scale(time), 
                        sigmaNoise = noise, 
                        k = SquaredExpKernel, 
                        sigmaf = sigmaf, 
                        ell = ell)

# Plotting results
plotPosterior(means = meanPredict, 
              variances = posteriorDay$posteriorVariance,
              x = time,
              y = tempSubset,
              XStar = time)

plot(time, tempSubset, type = "p")
lines(time, meanPredict, col = "red", lwd = 2)
lines(time, meanPredictDay, col = "blue", lwd = 2)
legend("bottomright", 
       legend=c("time", "day"),
       col=c("red", "blue"), lty=c(1,1,1,0), 
       pch=c(-1,-1,-1,19), lwd=c(2,2,2,0))



```

The pros with the model from (2) is that it takes temperature readings in close time proximity into account. This leads to the model being able to catch time periods that are particularly cold or warm compared to the same time period other years. The model from (4), which is shown by the blue line could however be better when predicting for years in the future if we assume that there is no trend in temperature that covers multiple years (what is global warming even??). If we have this assumption, the red line would wrongly predict cold temperatures for a coming year if the year previously was cold. In conclusion, the model that is best for future predictions depends on what assumptions we have on the data.

The variances for both the models seems to be somewhat similiar. 


# TASK 2.5
* Finally, implement a generalization of the periodic kernel given in the lectures:*

![Model](C:\Users\gusta\Desktop\Studier\TDDE15\Lab 4\eq3.png)

*Note that we have two different length scales here, and ell2 controls the correlation between the same day in different years. Estimate the GP model using the time variable
with this kernel and hyperparameters sigmaf = 20, ell1 = 1, ell2 = 10 and d = 365/sd(time).
The reason for the rather strange period here is that kernlab standardizes the inputs to have standard deviation of 1. Compare the fit to the previous two models (with
sigmaf = 20 and ell = 0.2). Discuss the results.*

```{r echo=TRUE, eval=TRUE}
periodicKernel <- function(sigmaf = 1, ell1 = 1, ell2 = 1, d) 
{
  rval <- function(x, y = NULL) {
    r = crossprod(x-y);
    exp1 = exp(-2*(sin(pi*sqrt(r)/d)**2)/ell1**2)
    exp2 = exp(-0.5*r*(ell2**2))
    return((sigmaf**2)*exp1*exp2)
  }
  class(rval) <- "kernel"
  return(rval)
  
}
```

```{r echo=FALSE, eval=TRUE}

lmFit = lm(tempSubset ~ time + time**2) 
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell1 = 1
ell2 = 10
d = sd(time)
kernel = periodicKernel(sigmaf = sigmaf, ell1 = ell1, ell2 = ell2, d)

#
fitGP = gausspr(x = time, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredictPer = predict(fitGP, time)

plot(time, tempSubset, type = "l")
lines(time, meanPredict, col = "red", lwd = 2)
lines(time, meanPredictDay, col = "blue", lwd = 2)
lines(time, meanPredictPer, col = "green", lwd = 2)
legend("bottomright", 
       legend=c("time", "day", "periodic"),
       col=c("red", "blue", "green"), lty=c(1,1,1,0), 
       pch=c(-1,-1,-1,19), lwd=c(2,2,2,0))

```

The perodic model, shown by the green line, seems to be fitting the data in the best way. One reason for this could be that this model, unlike the model in (4) and (2), takes into account that the data should be periodic but stills looks at readings in close time proximity. 

# TASK 3.1
*Use the R package kernlab to fit a Gaussian process classification model for fraud
on the training data. Use the default kernel and hyperparameters. Start using only
the covariates varWave and skewWave in the model. Plot contours of the prediction
probabilities over a suitable grid of values for varWave and skewWave. Overlay the
training data for fraud = 1 (as blue points) and fraud = 0 (as red points). You can reuse
code from the file KernLabDemo.R available on the course website. Compute the
confusion matrix for the classifier and its accuracy.*

```{r echo=FALSE, eval=TRUE}
library(kernlab)
library(AtmRay)

data <- read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/
GaussianProcess/Code/banknoteFraud.csv", header=FALSE, sep=",")

names(data) <- c("varWave","skewWave","kurtWave","entropyWave","fraud")

data[,5] <- as.factor(data[,5])

set.seed(111); 

SelectTraining <- sample(1:dim(data)[1], size = 1000, replace = FALSE)
SelectTest <- sample(1:dim(data)[1], size = 1000, replace = FALSE)

trainingData = data[SelectTraining,]
testingData = data[SelectTest,]

GPfit = gausspr(fraud ~  varWave + skewWave, data=trainingData)

x1 = seq(min(trainingData$varWave), max(trainingData$varWave), length=100)
x2 = seq(min(trainingData$skewWave), max(trainingData$skewWave), length=100)
gridXY = meshgrid(x1, x2)
gridXY <- data.frame(cbind(c(gridXY$x), c(gridXY$y)))
names(gridXY) <- c("varWave", "skewWave")
prediction = predict(GPfit, gridXY, type="probabilities")

# Plotting for Prob
contour(x1,x2,matrix(prediction[,1],100,byrow = TRUE), 20, xlab = "varWave", ylab = "skewWave")
points(trainingData[trainingData[, 5]==0, 1], trainingData[trainingData[, 5]==0, 2], col="red")
points(trainingData[trainingData[, 5]==1, 1], trainingData[trainingData[, 5]==1, 2], col="blue")

confusionMatrix = table(predict(GPfit,trainingData[,1:4]), trainingData[,5]) # confusion matrix
confusionMatrix
accuracy = sum(diag(confusionMatrix)) / sum(confusionMatrix)
accuracy
```

# TASK 3.2
* Using the estimated model from (1), make predictions for the test set. Compute the
accuracy.*

```{r echo=FALSE, eval=TRUE}
prediction = predict(GPfit, testingData)
confusionMatrix = table(prediction, testingData[,5]) # confusion matrix
confusionMatrix
accuracy = sum(diag(confusionMatrix)) / sum(confusionMatrix)
accuracy

```

The resulting accuarcy on the test set is the same as on the training set, however we can see that the confusion matrix has changed.

# TASK 3.3
*Train a model using all four covariates. Make predictions on the test set and compare
the accuracy to the model with only two covariates.*

```{r echo=FALSE, eval=TRUE}
GPfit = gausspr(fraud ~  varWave + skewWave + kurtWave + entropyWave, data=trainingData)

prediction = predict(GPfit, testingData)
confusionMatrix = table(prediction, testingData[,5]) # confusion matrix
confusionMatrix
accuracy = sum(diag(confusionMatrix)) / sum(confusionMatrix)
accuracy


```

Here we see that we get almost a hundred percent accuarcy when using all four covariates to make the predictions.

\pagebreak

## Appendix for code

```{r appendix, echo=TRUE, eval=FALSE}
## File Lab4_implement_GPR ##
library(kernlab)

# Code from earlier part of lab
library("mvtnorm")

# Covariance function
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA,n1,n2)
  for (i in 1:n2){
    K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
  }
  return(K)
}

# Gaussian Regression Model
# INPUTS:
#   X: Vector of training inputs.
#   y: Vector of training targets/outputs.
#   XStar: Vector of inputs where the posterior distribution is evaluated.
#   sigmaNoise: Noise standard deviation.
#   k: Covariance function or kernel. That is, the kernel should be a separate function
posteriorGP <- function(X, y, XStar, sigmaNoise, k, sigmaf, ell){
  
  # Compute covariance matrix K(X,X)
  covM = k(X, X, sigmaf, ell)
  
  # Identity matrix 
  I = diag(dim(covM)[1])
  
  # Transposing the output from the cholesky-function since the implementation in R returns 
  # an upper triangular matrix and we want a lower triangular matrix. 
  L = t(chol(covM + (sigmaNoise**2)*I))
  
  # Compute covariance matrix K(X,XStar)
  KStar = k(X, XStar, sigmaf, ell)
  # Predictive mean
  alpha = solve(t(L), solve(L, y))
  FStarMean = t(KStar)%*%alpha
  
  # Compute covariance matrix K(XStar,XStar)
  KStar2 = k(XStar, XStar, sigmaf, ell)
  # Predictive variance
  v = solve(L, KStar)
  V = diag(KStar2 - t(v)%*%v)
  
  # Compute log marginal likelihood
  #n = length(y)
  #logSum = 0
  #for(i in (1:n)) {
  #  logSum = logSum + log(L[i,i])
  #}
  
  # logProbYX = 0.5*t(y)*alpha - logSum - n/2*log(2*pi) # Not needed in this lab? Is a part of algorithm in 
  # Rasmussen and Williams' book
  logProbYX = 1
  
  return(list('posteriorMean' = FStarMean, 'posteriorVariance' = V, 'logProb' = logProbYX))
  
}

# Function for plotting posterior distributions with 95% probability band.
# INPUTS:   
#   means: Computed means for a number of points
#   variances: Computed variances for a number of points 
#   x: x-values for observations
#   y: y-values for observations
#   XStar: Vector of inputs where the posterior distribution is evaluated.
plotPosterior <- function(means, variances, x, y, XStar){
  var = sd(means) * variances + means
  upperBand = means + sqrt(var)*1.96
  lowerBand = means - sqrt(var)*1.96
  
  plot(x, y, type="p",
       xlim = c(min(x), max(x)), 
       ylim = c((min(min(lowerBand), min(y))), max(max(upperBand), max(y))))
  lines(XStar, means, col = "red", lwd = 2)
  lines(XStar, upperBand, col = "blue", lwd = 2)
  lines(XStar, lowerBand, col = "blue", lwd = 2)
  
}

# New code

data = read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/
Code/TempTullinge.csv", header=TRUE, sep=";")

# Creating variables with index for every fifth reading
tempTime = 1:2190
time = tempTime[seq(1,2190,5)]
dayNr = 1:365
tempDay = c()
for(i in 1:6){
  tempDay = c(tempDay, dayNr)
}
day = tempDay[seq(1,2190,5)]
temperature = unlist(data['temp'])
tempSubset = temperature[time]

# TASK 2.2.1

# Returns a square exponential kernel function
# INPUTS:
#   sigmaf: standard deviation for f
#   ell: Smoothing factor
squareExpKernel <- function(sigmaf = 1, ell = 1) 
{
  rval <- function(x, y = NULL) {
    r = crossprod(x-y);
    #return((sigmaf**2)*exp(-r/2*(ell**2)))
    return(sigmaf^2 * exp(-0.5 * r / ell^2))
  }
  class(rval) <- "kernel"
  return(rval)
  
}

# Creating kernel with sigmaf = 1, ell = 1
kernel = squareExpKernel(1,1)
# Evaluating in point x = 1, x' = 2
covM = kernel(1,2)

# Computing kernel matrix for created kernel and vectors X, XStar
X = c(1, 3, 4)
XStar = c(2, 3 ,4)
kernelMatrix = kernelMatrix(kernel = kernel, X, XStar)

# TASK 2.2.2 & 2.2.3

#
lmFit = lm(tempSubset ~ time + time**2) # vad Ã¤r detta exakt?
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell = 0.2
kernel = squareExpKernel(sigmaf = sigmaf, ell = ell)

#
fitGP = gausspr(x = time, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredict = predict(fitGP, time)

#
posterior = posteriorGP(X = scale(time), 
                        y = scale(tempSubset), 
                        XStar = scale(time), 
                        sigmaNoise = noise, 
                        k = SquaredExpKernel, 
                        sigmaf = sigmaf, 
                        ell = ell)

# Plotting results
plotPosterior(means = meanPredict, 
              variances = posterior$posteriorVariance,
              x = time,
              y = tempSubset,
              XStar = time)

# TASK 2.2.4

#
lmFit = lm(tempSubset ~ day + day**2) 
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell = 0.2
kernel = squareExpKernel(sigmaf = sigmaf, ell = ell)

#
fitGP = gausspr(x = day, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredictDay = predict(fitGP, day)

#
posteriorDay = posteriorGP(X = scale(day), 
                        y = scale(tempSubset), 
                        XStar = scale(time), 
                        sigmaNoise = noise, 
                        k = SquaredExpKernel, 
                        sigmaf = sigmaf, 
                        ell = ell)

# Plotting results
plotPosterior(means = meanPredict, 
              variances = posteriorDay$posteriorVariance,
              x = time,
              y = tempSubset,
              XStar = time)

plot(time, tempSubset, type = "l")
lines(time, meanPredict, col = "red", lwd = 2)
lines(time, meanPredictDay, col = "blue", lwd = 2)

# TASK 2.2.5

periodicKernel <- function(sigmaf = 1, ell1 = 1, ell2 = 1, d) 
{
  rval <- function(x, y = NULL) {
    r = crossprod(x-y);
    exp1 = exp(-2*(sin(pi*sqrt(r)/d)**2)/ell1**2)
    exp2 = exp(-0.5*r*(ell2**2))
    return((sigmaf**2)*exp1*exp2)
  }
  class(rval) <- "kernel"
  return(rval)
  
}

lmFit = lm(tempSubset ~ time + time**2) 
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell1 = 1
ell2 = 10
d = sd(time)
kernel = periodicKernel(sigmaf = sigmaf, ell1 = ell1, ell2 = ell2, d)

#
fitGP = gausspr(x = time, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredictPer = predict(fitGP, time)

#
#posteriorPer = posteriorGP1(X = scale(time), 
#                           y = scale(tempSubset), 
#                           XStar = scale(time), 
#                           sigmaNoise = noise, 
#                           k = periodicKernel, 
#                           sigmaf = sigmaf, 
#                           ell1 = ell1,
#                           ell2 = ell2)

# Plotting results
#plotPosterior(means = meanPredictPer, 
#              variances = posteriorPer$posteriorVariance,
#              x = time,
#              y = tempSubset,
#              XStar = time)

plot(time, tempSubset, type = "l")
lines(time, meanPredict, col = "red", lwd = 2)
lines(time, meanPredictDay, col = "blue", lwd = 2)
lines(time, meanPredictPer, col = "green", lwd = 2)

## File Lab4_reg_with_kernlab ##

library(kernlab)

# Code from earlier part of lab
library("mvtnorm")

# Covariance function
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA,n1,n2)
  for (i in 1:n2){
    K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
  }
  return(K)
}

# Gaussian Regression Model
# INPUTS:
#   X: Vector of training inputs.
#   y: Vector of training targets/outputs.
#   XStar: Vector of inputs where the posterior distribution is evaluated.
#   sigmaNoise: Noise standard deviation.
#   k: Covariance function or kernel. That is, the kernel should be a separate function
posteriorGP <- function(X, y, XStar, sigmaNoise, k, sigmaf, ell){
  
  # Compute covariance matrix K(X,X)
  covM = k(X, X, sigmaf, ell)
  
  # Identity matrix 
  I = diag(dim(covM)[1])
  
  # Transposing the output from the cholesky-function since the implementation in R returns 
  # an upper triangular matrix and we want a lower triangular matrix. 
  L = t(chol(covM + (sigmaNoise**2)*I))
  
  # Compute covariance matrix K(X,XStar)
  KStar = k(X, XStar, sigmaf, ell)
  # Predictive mean
  alpha = solve(t(L), solve(L, y))
  FStarMean = t(KStar)%*%alpha
  
  # Compute covariance matrix K(XStar,XStar)
  KStar2 = k(XStar, XStar, sigmaf, ell)
  # Predictive variance
  v = solve(L, KStar)
  V = diag(KStar2 - t(v)%*%v)
  
  # Compute log marginal likelihood
  #n = length(y)
  #logSum = 0
  #for(i in (1:n)) {
  #  logSum = logSum + log(L[i,i])
  #}
  
  # logProbYX = 0.5*t(y)*alpha - logSum - n/2*log(2*pi) # Not needed in this lab? Is a part of algorithm in 
  # Rasmussen and Williams' book
  logProbYX = 1
  
  return(list('posteriorMean' = FStarMean, 'posteriorVariance' = V, 'logProb' = logProbYX))
  
}

# Function for plotting posterior distributions with 95% probability band.
# INPUTS:   
#   means: Computed means for a number of points
#   variances: Computed variances for a number of points 
#   x: x-values for observations
#   y: y-values for observations
#   XStar: Vector of inputs where the posterior distribution is evaluated.
plotPosterior <- function(means, variances, x, y, XStar){
  var = sd(means) * variances + means
  upperBand = means + sqrt(var)*1.96
  lowerBand = means - sqrt(var)*1.96
  
  plot(x, y, type="p",
       xlim = c(min(x), max(x)), 
       ylim = c((min(min(lowerBand), min(y))), max(max(upperBand), max(y))))
  lines(XStar, means, col = "red", lwd = 2)
  lines(XStar, upperBand, col = "blue", lwd = 2)
  lines(XStar, lowerBand, col = "blue", lwd = 2)
  
}

# New code

data = read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/
Code/TempTullinge.csv", header=TRUE, sep=";")

# Creating variables with index for every fifth reading
tempTime = 1:2190
time = tempTime[seq(1,2190,5)]
dayNr = 1:365
tempDay = c()
for(i in 1:6){
  tempDay = c(tempDay, dayNr)
}
day = tempDay[seq(1,2190,5)]
temperature = unlist(data['temp'])
tempSubset = temperature[time]

# TASK 2.2.1

# Returns a square exponential kernel function
# INPUTS:
#   sigmaf: standard deviation for f
#   ell: Smoothing factor
squareExpKernel <- function(sigmaf = 1, ell = 1) 
{
  rval <- function(x, y = NULL) {
    r = crossprod(x-y);
    #return((sigmaf**2)*exp(-r/2*(ell**2)))
    return(sigmaf^2 * exp(-0.5 * r / ell^2))
  }
  class(rval) <- "kernel"
  return(rval)
  
}

# Creating kernel with sigmaf = 1, ell = 1
kernel = squareExpKernel(1,1)
# Evaluating in point x = 1, x' = 2
covM = kernel(1,2)

# Computing kernel matrix for created kernel and vectors X, XStar
X = c(1, 3, 4)
XStar = c(2, 3 ,4)
kernelMatrix = kernelMatrix(kernel = kernel, X, XStar)

# TASK 2.2.2 & 2.2.3

#
lmFit = lm(tempSubset ~ time + time**2) # vad Ã¤r detta exakt?
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell = 0.2
kernel = squareExpKernel(sigmaf = sigmaf, ell = ell)

#
fitGP = gausspr(x = time, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredict = predict(fitGP, time)

#
posterior = posteriorGP(X = scale(time), 
                        y = scale(tempSubset), 
                        XStar = scale(time), 
                        sigmaNoise = noise, 
                        k = SquaredExpKernel, 
                        sigmaf = sigmaf, 
                        ell = ell)

# Plotting results
plotPosterior(means = meanPredict, 
              variances = posterior$posteriorVariance,
              x = time,
              y = tempSubset,
              XStar = time)

# TASK 2.2.4

#
lmFit = lm(tempSubset ~ day + day**2) 
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell = 0.2
kernel = squareExpKernel(sigmaf = sigmaf, ell = ell)

#
fitGP = gausspr(x = day, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredictDay = predict(fitGP, day)

#
posteriorDay = posteriorGP(X = scale(day), 
                        y = scale(tempSubset), 
                        XStar = scale(time), 
                        sigmaNoise = noise, 
                        k = SquaredExpKernel, 
                        sigmaf = sigmaf, 
                        ell = ell)

# Plotting results
plotPosterior(means = meanPredict, 
              variances = posteriorDay$posteriorVariance,
              x = time,
              y = tempSubset,
              XStar = time)

plot(time, tempSubset, type = "l")
lines(time, meanPredict, col = "red", lwd = 2)
lines(time, meanPredictDay, col = "blue", lwd = 2)

# TASK 2.2.5

periodicKernel <- function(sigmaf = 1, ell1 = 1, ell2 = 1, d) 
{
  rval <- function(x, y = NULL) {
    r = crossprod(x-y);
    exp1 = exp(-2*(sin(pi*sqrt(r)/d)**2)/ell1**2)
    exp2 = exp(-0.5*r*(ell2**2))
    return((sigmaf**2)*exp1*exp2)
  }
  class(rval) <- "kernel"
  return(rval)
  
}

lmFit = lm(tempSubset ~ time + time**2) # vad Ã¤r detta exakt?
noise = sd(lmFit$residuals) 

# Creating kernel with sigmaf = 20, ell = 0.2
sigmaf = 20
ell1 = 1
ell2 = 10
d = sd(time)
kernel = periodicKernel(sigmaf = sigmaf, ell1 = ell1, ell2 = ell2, d)

#
fitGP = gausspr(x = time, 
                y = tempSubset, 
                kernel = kernel, 
                var = noise**2)
meanPredictPer = predict(fitGP, time)

#
#posteriorPer = posteriorGP1(X = scale(time), 
#                           y = scale(tempSubset), 
#                           XStar = scale(time), 
#                           sigmaNoise = noise, 
#                           k = periodicKernel, 
#                           sigmaf = sigmaf, 
#                           ell1 = ell1,
#                           ell2 = ell2)

# Plotting results
#plotPosterior(means = meanPredictPer, 
#              variances = posteriorPer$posteriorVariance,
#              x = time,
#              y = tempSubset,
#              XStar = time)

plot(time, tempSubset, type = "l")
lines(time, meanPredict, col = "red", lwd = 2)
lines(time, meanPredictDay, col = "blue", lwd = 2)
lines(time, meanPredictPer, col = "green", lwd = 2)


```